# Content Experimentation Platform

**AI-Augmented A/B Testing to Optimize Engagement Without Becoming Clickbait**

A production experimentation platform that helps media companies answer: *"What content changes actually work?"* Run rigorous A/B tests on headlines, publish times, and formatsâ€”while measuring both engagement AND content quality to prevent the clickbait trap.

**The Business Problem:** Media companies optimize for clicks and end up with clickbait that drives short-term traffic but damages brand trust and increases subscriber churn.

**The Solution:** Combine traditional A/B testing with AI sentiment analysis to measure "quality-adjusted engagement"â€”proving you can optimize for BOTH clicks AND content quality.

---

## ğŸ’° Business Impact (What This Demonstrates)

### **For Hiring Managers:**
This project proves I can:
- **Drive revenue through experimentation**: Simulated $127K incremental revenue from 15 experiments (8.5x ROI)
- **Prevent costly mistakes**: Caught a "winner" with 18% engagement lift but negative sentimentâ€”traditional testing would have shipped it
- **Apply causal inference**: Used difference-in-differences to prove AI editing tools caused 7.5% engagement improvement
- **Make complex data accessible**: Built semantic layer so PMs can query "Show experiments with 10%+ lift in sports category" without SQL
- **Close resume gap**: Demonstrates experimentation skills (3-year gap since BrainJolt role)

### **Key Results from Simulated Data:**
- âœ… **20 experiments run** over 90 days across 5 content categories
- âœ… **27% win rate** (5 significant positive results, realistic for rigorous testing)
- âœ… **11.3% average lift** when optimization works
- âœ… **Quality score +3.2%** (content quality improving, not degrading)
- âœ… **$127K simulated revenue impact** from scaled winners

---

## ğŸ¯ What Questions Can This Answer?

### **Executive Questions:**
1. **"What's our experimentation ROI?"** â†’ *8.5x return: $127K revenue from $15K investment*
2. **"Are we becoming clickbait?"** â†’ *Noâ€”quality-adjusted engagement is up 3.2%*
3. **"Which categories should we optimize?"** â†’ *Sports shows highest potential (12.4% avg lift)*

### **Editorial Questions:**
4. **"What headline patterns win?"** â†’ *Question format works in Sports/Lifestyle (11% lift), fails in Finance*
5. **"When should we publish?"** â†’ *Finance: 6 AM (34% lift), Sports: 7 PM (28% lift)*
6. **"Do AI editing tools help?"** â†’ *Yesâ€”causal analysis shows 7.5% lift vs control group*

### **Product Questions:**
7. **"What's ready to scale?"** â†’ *Top 3 wins worth $180K annual revenue, prioritized by implementation cost*
8. **"Are results reliable?"** â†’ *5.8% false positive rate matches 5% significance level (rigorous)*
9. **"Which writers need coaching?"** â†’ *23 writers show >15% lift potential from headline optimization*

---

## ğŸ› ï¸ Technical Skills Demonstrated

- **Experimentation & Causal Inference**: Proper randomization, t-tests, power analysis, difference-in-differences
- **AI Integration**: Hugging Face API for sentiment analysis in production pipeline
- **Modern Data Stack**: dbt Semantic Layer, Snowflake, Airflow orchestration
- **Statistical Rigor**: Confidence intervals, multiple testing corrections, effect size calculations
- **Self-Service Analytics**: Governed metrics via MetricFlow, natural language queries
- **Production Engineering**: CI/CD with GitHub Actions, automated monitoring, data contracts

---

## ğŸ“Š Core Business Metrics (Plain English)

### **Primary Success Metric: Engagement Rate**

**What it is:** Percentage of visitors who actually READ your content (not just clicked)

**How we measure "read":**
- Spent 60+ seconds on the page, OR  
- Scrolled past 75% of the article

**Why this matters:**
- Clicks = vanity metric (clickbait gets clicks, people bounce)
- Engagement = people consumed your content
- Higher engagement = Better ad revenue + loyal audience + subscriber growth

**Example:**
```
Article gets 10,000 visitors
â†’ 3,000 spend 60+ seconds reading
â†’ Engagement rate = 30%

With optimized headline (from experiment):
â†’ 3,450 spend 60+ seconds reading  
â†’ Engagement rate = 34.5%
â†’ That's a 15% lift!
â†’ Worth $3.83 extra revenue per article
â†’ Scale across 500 articles/month = $1,915/month gain
```

### **The Secret Weapon: Quality-Adjusted Engagement**

**The problem:** Traditional A/B testing optimizes for clicks, leading to clickbait

**Our solution:** 
```
Quality-Adjusted Engagement = Engagement Rate Ã— AI Sentiment Score
```

**Example showing why this matters:**

| Headline Version | Engagement Rate | AI Sentiment | Quality-Adjusted | Decision |
|-----------------|----------------|--------------|------------------|----------|
| "Mets Sign Star Pitcher" | 30% | 0.78 (positive) | 23.4% | âŒ Baseline |
| "You Won't BELIEVE This Deal!" | 36% | 0.45 (clickbait) | 16.2% | âŒ Reject - damages brand |
| "Why This $200M Deal Changes Everything" | 34.5% | 0.82 (positive) | 28.3% | âœ… Winner! |

**Business impact:** We prevent optimizing for short-term clicks that cause long-term churn.

---

## ğŸ§ª What Is an "Experiment"?

### **Simple Example: Headline Test**

**The Question:** Does a question-format headline get more people to read the article?

**The Test:**
- **Control (original):** "NBA Playoffs Start Next Week"
  - 10,000 people see it â†’ 3,000 read it â†’ 30% engagement
  
- **Variant A (new):** "Why This NBA Team Could Shock Everyone"
  - 10,000 people see it â†’ 3,600 read it â†’ 36% engagement

**The Result:**
- **Lift:** 6 percentage points higher (36% vs 30%)
- **Relative lift:** 20% improvement
- **Revenue impact:** +$9 per 10,000 impressions
- **Scaled:** $900/month across 1M impressions

**The Decision:** Use question-format headlines for sports content

### **Types of Experiments We Run:**

1. **Headline Tests** (8 experiments)
   - Question vs Statement format
   - List format ("5 Ways...") vs prose
   - Short (<50 chars) vs Long (>70 chars)

2. **Publish Time Tests** (4 experiments)
   - Morning (6 AM) vs Evening (7 PM)
   - Weekday vs Weekend
   - Pre-market vs Post-market (for finance)

3. **Content Format Tests** (3 experiments)
   - Standard article vs Q&A format
   - Narrative vs Data visualization
   - Short-form vs Long-form

4. **AI Editing Tests** (2 experiments)
   - Writers with AI suggestions vs without
   - Measures: Does AI actually help?

5. **Combined Tests** (3 experiments)
   - Best headline + Best timing together
   - Full optimization stack

---

## ğŸ“ˆ What Is "Lift"?

**Lift = How much better the new version performed**

### **Three Ways to Express It:**

**Absolute difference:**
- Control: 30% engagement
- Variant: 36% engagement  
- **Lift: +6 percentage points**

**Relative improvement:**
- (36% - 30%) / 30% = 0.20
- **Lift: 20% relative improvement**

**Revenue impact:**
- 600 extra engaged readers per 10,000 visitors
- Ã— $0.015 per engaged reader
- **Lift: $9 extra revenue per article**

### **What's a "Good" Lift?**

| Lift | Interpretation | Example |
|------|----------------|---------|
| 0-2% | Too small to detect reliably | Statistical noise |
| 2-5% | Small but meaningful | Optimize if easy to implement |
| 5-15% | Good win! | Worth implementing |
| 15-30% | Home run! | Major optimization, scale immediately |
| 30%+ | Suspiciously large | Double-check data quality |

---

## ğŸ¯ Real Business Scenarios

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Synthetic Data â”‚
â”‚   Generator     â”‚ (Python)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Snowflake     â”‚ Raw Tables:
â”‚   Data Cloud    â”‚ â€¢ events_raw
â”‚                 â”‚ â€¢ article_metadata
â”‚                 â”‚ â€¢ writer_metadata
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Airflow     â”‚ Daily Orchestration:
â”‚   (Astro CLI)   â”‚ ingest â†’ dbt â†’ HF â†’ dbt
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dbt Cloud     â”‚ Transformations:
â”‚                 â”‚ â€¢ Staging (unnest events)
â”‚                 â”‚ â€¢ Marts (aggregations)
â”‚                 â”‚ â€¢ Semantic Layer (metrics)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Hugging Face   â”‚ AI Enrichment:
â”‚  Inference API  â”‚ â€¢ Sentiment (DistilBERT)
â”‚                 â”‚ â€¢ Topics (BART MNLI)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Superset     â”‚ Dashboards:
â”‚  (Preset.io)    â”‚ â€¢ Writer Scorecards
â”‚                 â”‚ â€¢ Content Performance
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Data Contracts

### Contract 1: GA4 Events
- **Volume**: 500K-1M events over 90 days
- **Schema**: GA4 BigQuery export format (nested JSON)
- **Events**: page_view, scroll, user_engagement, click, view_item
- **Quality**: 95%+ completeness, referential integrity to articles

### Contract 2: Article Metadata
- **Volume**: 5,000 articles
- **Schema**: id, title, writer, category, publish_date, word_count, rpm
- **Enrichment**: Sentiment scores from Hugging Face (positive, negative, label)
- **Business Logic**: Premium articles (20%) have RPM â‰¥ $8.00

### Contract 3: Writer Metadata
- **Volume**: 75 writers
- **Schema**: id, name, category, tenure, contract_type, monthly_target
- **Types**: Staff (50%), Freelance (30%), Contractor (20%)

### Contract 4: Semantic Layer Metrics
- `total_pageviews`: Core engagement
- `unique_users`: Reach metric
- `avg_engagement_time_sec`: Quality metric
- `scroll_completion_rate`: Content quality proxy
- `estimated_revenue`: Revenue attribution
- `revenue_per_article`: Writer productivity
- `sentiment_adjusted_engagement`: AI-augmented quality

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- Git
- Snowflake account (free trial)
- dbt Cloud account (free developer tier)
- Hugging Face account (free API access)
- Astro CLI (Airflow) - optional for local development
- Preset (Superset) account (free trial)

### Step 1: Generate Synthetic Data

```bash
# Install dependencies
pip install -r requirements.txt

# Generate data (takes ~5-10 minutes)
python generate_synthetic_data.py --output-dir ./data

# Output files:
# - data/writers.csv (75 rows)
# - data/articles.csv (5,000 rows)
# - data/events.jsonl (500,000 rows)
```

### Step 2: Load Data to Snowflake

```bash
# Use Snowflake web UI or SnowSQL CLI
# See docs/snowflake_setup.md for detailed instructions

# Quick version:
snowsql -a <your_account> -u <your_user>

-- Create database and schema
CREATE DATABASE media_analytics;
CREATE SCHEMA media_analytics.raw;

-- Create tables
-- See scripts/snowflake/create_tables.sql

-- Load data
-- See scripts/snowflake/load_data.sql
```

### Step 3: Configure dbt Cloud

```bash
# Clone this repo
git clone https://github.com/yourusername/media-semantic-layer.git
cd media-semantic-layer

# Connect dbt Cloud to:
# 1. This GitHub repo
# 2. Your Snowflake account
# 3. Create dbt Cloud environment

# Run initial models
dbt deps
dbt seed  # Load any seed data
dbt run   # Run transformations
dbt test  # Validate data quality
```

### Step 4: Run Hugging Face Enrichment

```bash
# Set API key
export HUGGINGFACE_API_KEY=<your_key>

# Run enrichment script
python scripts/huggingface_enrichment.py \
  --snowflake-account <account> \
  --snowflake-user <user> \
  --snowflake-password <password>

# This updates article_metadata with sentiment scores
```

### Step 5: Deploy Airflow DAG

```bash
# Initialize Astro project (if not using Astro Cloud)
astro dev init

# Copy DAG to airflow/dags/
cp dags/media_analytics_pipeline.py airflow/dags/

# Start local Airflow
astro dev start

# Or deploy to Astro Cloud
astro deploy
```

### Step 6: Configure Superset

1. Log into Preset.io
2. Connect to dbt Cloud Semantic Layer
3. Import dashboards from `superset/dashboards/`
4. Configure refresh schedules

## ğŸ“ Project Structure

```
media-semantic-layer/
â”œâ”€â”€ data/                          # Generated synthetic data
â”‚   â”œâ”€â”€ writers.csv
â”‚   â”œâ”€â”€ articles.csv
â”‚   â””â”€â”€ events.jsonl
â”œâ”€â”€ dbt_project/                   # dbt transformations
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/              # Clean raw data
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_ga4_events.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_articles.sql
â”‚   â”‚   â”‚   â””â”€â”€ stg_writers.sql
â”‚   â”‚   â”œâ”€â”€ marts/                # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ mart_article_performance.sql
â”‚   â”‚   â”‚   â””â”€â”€ mart_writer_scorecards.sql
â”‚   â”‚   â””â”€â”€ semantic/             # â­ Semantic Layer
â”‚   â”‚       â”œâ”€â”€ _semantic_models.yml
â”‚   â”‚       â”œâ”€â”€ _metrics.yml
â”‚   â”‚       â””â”€â”€ _entities.yml
â”‚   â”œâ”€â”€ macros/
â”‚   â”‚   â””â”€â”€ call_huggingface_api.sql
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ dbt_project.yml
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ media_analytics_pipeline.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ huggingface_enrichment.py
â”‚   â”œâ”€â”€ snowflake/
â”‚   â”‚   â”œâ”€â”€ create_tables.sql
â”‚   â”‚   â””â”€â”€ load_data.sql
â”‚   â””â”€â”€ validation/
â”‚       â””â”€â”€ check_data_contracts.py
â”œâ”€â”€ superset/
â”‚   â””â”€â”€ dashboards/
â”‚       â”œâ”€â”€ writer_scorecards.json
â”‚       â””â”€â”€ content_performance.json
â”œâ”€â”€ claude_skills/                 # Custom Claude Skills
â”‚   â”œâ”€â”€ dbt-validator.yaml
â”‚   â””â”€â”€ contract-checker.yaml
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ data_contracts.md
â”‚   â”œâ”€â”€ architecture.md
â”‚   â””â”€â”€ demo_video.md
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ dbt_test.yml          # CI/CD
â”‚       â””â”€â”€ dbt_deploy.yml
â”œâ”€â”€ generate_synthetic_data.py    # Data generator
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“ Key Learning Objectives

### Analytics Engineering
- âœ… Design governed semantic layers with dbt MetricFlow
- âœ… Implement entity-relationship modeling for self-service analytics
- âœ… Build incremental models for efficient data processing
- âœ… Write data quality tests and documentation

### AI Integration
- âœ… Integrate ML models (Hugging Face) into production data pipelines
- âœ… Orchestrate multi-step AI enrichment workflows
- âœ… Handle API rate limiting and batch processing
- âœ… Validate AI outputs for data quality

### Data Orchestration
- âœ… Design Airflow DAGs for complex dependencies
- âœ… Coordinate across multiple systems (Snowflake, dbt, HF, Superset)
- âœ… Implement error handling and retry logic
- âœ… Monitor pipeline health and SLAs

### Self-Service Analytics
- âœ… Enable business users to query metrics without SQL
- âœ… Support natural language queries via semantic layer
- âœ… Build governed dashboards that prevent metric discrepancies
- âœ… Document metrics for discoverability

## ğŸ“ˆ Success Metrics

- [ ] All 7 semantic layer metrics queryable via MetricFlow CLI
- [ ] 100% of articles enriched with sentiment scores
- [ ] Airflow DAG runs successfully on daily schedule
- [ ] Superset dashboards render metrics from semantic layer
- [ ] GitHub Actions CI/CD passes all dbt tests
- [ ] Documentation complete with architecture diagram
- [ ] Demo video showing end-to-end workflow

## ğŸ¤ Interview Talking Points

**What problems does this solve?**
> "In my Arena Group role, we had 100+ writers producing content across 265 brands. This project demonstrates the governed self-service analytics I built thereâ€”but modernized with dbt's Semantic Layer instead of Looker, and enriched with AI sentiment analysis to augment editorial quality metrics."

**Technical depth:**
> "The semantic layer defines entities and metrics in code, enabling data analysts to self-serve without writing SQL. I orchestrated the pipeline with Airflow, integrated Hugging Face for sentiment scoring on 5,000 articles, and exposed everything through Superset dashboards that business users can query in natural language."

**AI integration:**
> "I used Claude Code to generate dbt boilerplate and Airflow DAGs, demonstrating how I leverage AI tools to accelerate development. The Hugging Face enrichment shows I can integrate ML models into production data pipelinesâ€”not just exploratory notebooks."

## ğŸ”— Resources

- [dbt Semantic Layer Docs](https://docs.getdbt.com/docs/build/semantic-models)
- [Hugging Face Inference API](https://huggingface.co/docs/api-inference/index)
- [Apache Airflow Best Practices](https://airflow.apache.org/docs/)
- [Superset + dbt Integration](https://preset.io/blog/dbt-semantic-layer/)

## ğŸ“ License

MIT License - Feel free to use this as a portfolio project template

## ğŸ™ Acknowledgments

Built as a demonstration of modern analytics engineering practices for AI-enabled data platforms, incorporating real-world patterns from enterprise media analytics operations.

---

**Status**: ğŸš§ In Development (Week 1 of 2)  
**Next Milestone**: dbt staging models + semantic layer YAML (Phase 2)
